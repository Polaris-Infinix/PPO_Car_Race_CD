import gymnasium as gym 
import numpy as np 
import torch
import torch.nn as nn 
import torch.optim as optim 
from torch.distributions import Categorical
import wandb
from environment_handler import Environment


env = Environment()
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
normalize = True
learning_rate = 3e-4
mini_batch = 256*2
batch_size = 4096
epoch = 5
clipping_eps = 0.2
wb=True 
load=False
entropy_offset=0
if wb:
    wandb.init(
        project="Lunar_lander",          
        name="run-112",              
        config={
            "act_learning_rate": learning_rate,
            "critic_learning_rate":learning_rate,
            "batch_size": batch_size,
            "update_epochs": 10,
            "clip_coef": 0.2,
            "env": "Lunar_lander",
            "Normalize":normalize,
            "mini batch size":mini_batch,
            "done":0
        }
    )
def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
    torch.nn.init.orthogonal_(layer.weight, std)
    torch.nn.init.constant_(layer.bias, bias_const)
    return layer

class agent(nn.Module):
    def __init__(self):
        super(agent,self).__init__()
        self.actor = nn.Sequential(
            layer_init(nn.Conv2d(12,20,5,padding=1,stride=2)),
            nn.ReLU(),
            layer_init(nn.Conv2d(20,40,4,2,1)),
            nn.ReLU(),
            layer_init(nn.Conv2d(40,64,3,2,1)),
            nn.ReLU(),
            nn.Flatten(),
            layer_init(nn.Linear(9216,512)),
            nn.ReLU(),
            layer_init(nn.Linear(512,200)),
            nn.ReLU(),
            layer_init(nn.Linear(200,5))
        )
        self.critic = nn.Sequential(
            layer_init(nn.Conv2d(12,20,5,padding=1,stride=2)),
            nn.ReLU(),
            layer_init(nn.Conv2d(20,40,4,2,1)),
            nn.ReLU(),
            layer_init(nn.Conv2d(40,64,3,2,1)),
            nn.ReLU(),
            nn.Flatten(),
            layer_init(nn.Linear(9216,512)),
            nn.ReLU(),
            layer_init(nn.Linear(512,1))
        )
    
    @torch.no_grad()
    def act(self, state):
        dist = Categorical(logits=self.actor(state))
        action = dist.sample()
        log_prob = dist.log_prob(action).unsqueeze(-1)
        value = self.critic(state)
        return action, log_prob, value
    
    def get_actions_probs(self, obs, action):
        dist = Categorical(logits=self.actor(obs))
        log_prob = dist.log_prob(action).unsqueeze(-1)
        entropy = dist.entropy().unsqueeze(-1)
        value = self.critic(obs)
        return log_prob, entropy, value

def compute_gae(rewards, values, dones, next_value, gamma=0.99, lam=0.95):
    
    advantages = torch.zeros_like(rewards)
    last_gae = 0
    for t in reversed(range(rewards.size(0))):
        mask = 1.0 - dones[t]
        delta = rewards[t] + gamma * next_value * mask - values[t]
        last_gae = delta + gamma * lam * last_gae * mask
        advantages[t] = last_gae
        next_value = values[t]

    returns = advantages + values
    return advantages, returns

model = agent().to(device)
optimizer = optim.Adam(model.parameters(), lr=learning_rate, eps=1e-5) 
global_rewards=[22]

if load:
    checkpoint = torch.load("ppo_model400.pt", map_location=device)
    model.actor.load_state_dict(checkpoint['actor_state_dict'])
    model.critic.load_state_dict(checkpoint['critic_state_dict'])


state = env.reset().to(device)

for gen in range(4000):
    #Tensors for storage
    states = torch.zeros(batch_size,12,96,96).to(device)
    actions = torch.zeros(batch_size,).to(device)
    rewards = torch.zeros(batch_size, 1).to(device)
    values = torch.zeros(batch_size, 1).to(device)
    log_probs = torch.zeros(batch_size, 1).to(device)
    dones = torch.zeros(batch_size, 1).to(device)
    
    for t in range(batch_size):
        with torch.no_grad():
            action, log_prob, value = model.act(state.unsqueeze(0))
        next_state, reward, terminated, truncated = env.input(action.item())
 
        done = terminated or truncated

        states[t] = state
        actions[t] = action
        rewards[t] = reward
        values[t] = value
        log_probs[t] = log_prob
        dones[t] = float(done)
       
        state = next_state.to(device)
        if done:
            state = env.reset().to(device)
    
    with torch.no_grad():
        next_val = model.critic(state.unsqueeze(0))

    
    
    entropy_coef=-(0.01/3775)*gen+0.01
    # if sum(global_rewards)/len(global_rewards)<0 and gen%5==0:
    #     entropy_offset=-0.001*(sum(global_rewards)/len(global_rewards))+0.13
    #     global_rewards=[-22]
    #     print(entropy_offset)
    # else:
    # #     entropy_offset==0
    advantages, returns = compute_gae(rewards, values, dones, next_val)
    entropy_offset=0
    
    if normalize:
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
    
    for _ in range(epoch):
        indices = torch.randperm(batch_size, device=device)
        
        for start in range(0, batch_size, mini_batch):
            batch_indices = indices[start:start+mini_batch]
  
            state_batch = states[batch_indices]
            action_batch = actions[batch_indices]
            log_prob_batch = log_probs[batch_indices]
            advantage_batch = advantages[batch_indices]
            return_batch = returns[batch_indices]

            new_probs, entropy, value = model.get_actions_probs(state_batch, action_batch)
            ratio = torch.exp(new_probs - log_prob_batch)
            clipped_ratio = torch.clamp(ratio, 1 - clipping_eps, 1 + clipping_eps)

            actor_loss = -torch.min(ratio * advantage_batch, clipped_ratio * advantage_batch).mean()
            
            critic_loss = (return_batch - value).pow(2).mean()
            
            entropy_loss = entropy.mean()

            total_loss = actor_loss + 0.5 * critic_loss - (entropy_coef * entropy_loss)

            optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
            optimizer.step()

    global_rewards.append(rewards.sum())
    print(f"Gen: {gen:4d} | Returns: {returns.mean():6.0f} | Rewards: {rewards.sum():4.0f} | " +
          f"Actor loss: {actor_loss:0.2f} | Critic loss: {critic_loss:0.2f} | " +
          f"Entropy: {entropy_loss:1.2f}")
    if wb:
        wandb.log({
        "returns":returns.mean(),
        "reward":rewards.sum(),
        # "total_loss" : sum(tl) / len(tl),
        "Actor_loss":actor_loss,
        "critic_loss":critic_loss,
        "Entropy":entropy_loss   

    })
    if gen%100==0:
        # Assume model is your PPO agent class
        torch.save({
        'actor_state_dict': model.actor.state_dict(),
        'critic_state_dict': model.critic.state_dict(),
        }, f"ppo_model{gen}.pt")

